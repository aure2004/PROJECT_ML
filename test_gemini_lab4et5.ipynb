{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0819eb7f",
   "metadata": {},
   "source": [
    "# LAB4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ecf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import warnings\n",
    "\n",
    "# Ignorer les avertissements pour plus de clarté\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"#######################################################\")\n",
    "print(\"### PART 1 (LAB 4): EXPLORATION, PREPROCESSING & BASELINE ###\")\n",
    "print(\"#######################################################\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. DESCRIPTIVE ANALYSIS\n",
    "# ==========================================\n",
    "print(\"\\n--- 1.1 Data Loading & Cleaning ---\")\n",
    "\n",
    "try:\n",
    "    # Chargement du fichier spécifique\n",
    "    filename = 'electric_vehicles_spec_2025.csv.csv'\n",
    "    # Le fichier semble utiliser la virgule comme séparateur standard\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Nettoyage des noms de colonnes : suppression des espaces et minuscules\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    \n",
    "    print(f\"File '{filename}' loaded successfully.\")\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    \n",
    "    # Nettoyage spécifique pour ce dataset : 'cargo_volume_l' est souvent objet\n",
    "    if 'cargo_volume_l' in df.columns and df['cargo_volume_l'].dtype == 'object':\n",
    "        print(\"Cleaning 'cargo_volume_l' column...\")\n",
    "        df['cargo_volume_l'] = pd.to_numeric(df['cargo_volume_l'], errors='coerce')\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{filename}' was not found. Please upload it.\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\n--- 1.2 General Information ---\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n--- 1.3 Descriptive Statistics (Numerical) ---\")\n",
    "# Sélection des colonnes numériques\n",
    "numerical_cols_desc = df.select_dtypes(include=np.number).columns.tolist()\n",
    "print(df[numerical_cols_desc].describe())\n",
    "\n",
    "print(\"\\n--- 1.4 Visualizations ---\")\n",
    "\n",
    "# Distribution de la cible : Range\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df['range_km'], kde=True, bins=30, color='blue')\n",
    "plt.title('Distribution of Target: Range (km)')\n",
    "plt.xlabel('Range (km)')\n",
    "plt.show()\n",
    "\n",
    "# Distribution d'une caractéristique clé : Capacité Batterie\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(df['battery_capacity_kwh'], kde=True, bins=30, color='green')\n",
    "plt.title('Distribution of Battery Capacity (kWh)')\n",
    "plt.xlabel('Capacity (kWh)')\n",
    "plt.show()\n",
    "\n",
    "# Analyse des variables catégorielles\n",
    "# On adapte les noms de colonnes à ce dataset spécifique\n",
    "categorical_cols_viz = ['brand', 'car_body_type', 'segment', 'drivetrain']\n",
    "for col in categorical_cols_viz:\n",
    "    if col in df.columns:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        # Top 15 pour éviter de surcharger le graphique\n",
    "        top_n = 15\n",
    "        counts = df[col].value_counts()\n",
    "        actual_n = min(top_n, len(counts))\n",
    "        top_categories = counts.nlargest(actual_n).index\n",
    "        \n",
    "        sns.countplot(data=df[df[col].isin(top_categories)], y=col, order=top_categories, palette='viridis')\n",
    "        plt.title(f'Top {actual_n} Counts for {col}')\n",
    "        plt.show()\n",
    "        print(f\"\\nTop counts for {col}:\\n{counts.head(actual_n)}\")\n",
    "\n",
    "# Ajout spécifique pour le Lab 4 : Boxplots pour les outliers\n",
    "print(\"\\n--- Visualization: Boxplots (Outlier Detection) ---\")\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.boxplot(x=df['range_km'])\n",
    "plt.title('Boxplot of Range (km)')\n",
    "plt.show()\n",
    "\n",
    "# Matrice de Corrélation\n",
    "plt.figure(figsize=(14, 10))\n",
    "corr_matrix = df.select_dtypes(include=np.number).corr()\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix (Numerical Features)')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 features correlated with 'range_km':\")\n",
    "print(corr_matrix['range_km'].sort_values(ascending=False).head(10))\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 2. PRE-PROCESSING IMPLEMENTATION\n",
    "# ==========================================\n",
    "print(\"\\n--- 2. Pre-processing Implementation ---\")\n",
    "\n",
    "# 1. Définir Cible (y) et Caractéristiques (X)\n",
    "target = 'range_km'\n",
    "# On retire la cible et les colonnes inutiles ou trop complexes pour la baseline\n",
    "drop_cols = [target, 'source_url'] \n",
    "if 'model' in df.columns:\n",
    "    drop_cols.append('model') # Trop de cardinalité pour une baseline simple\n",
    "\n",
    "X = df.drop(columns=drop_cols, errors='ignore')\n",
    "y = df[target]\n",
    "\n",
    "# 2. Identifier les types de caractéristiques\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "print(f\"Numerical Features ({len(numerical_features)}): {numerical_features}\")\n",
    "print(f\"Categorical Features ({len(categorical_features)}): {categorical_features}\")\n",
    "\n",
    "# 3. Définir les transformateurs\n",
    "# Numérique : Imputation (Médiane) + Scaling\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Catégoriel : Imputation (Constante) + Encodage OneHot\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# 4. Créer le ColumnTransformer (Le Préprocesseur)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# 5. Division Train-Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Data Split: Train shape {X_train.shape}, Test shape {X_test.shape}\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. PROBLEM FORMALIZATION\n",
    "# ==========================================\n",
    "print(\"\\n--- 3. Problem Formalization ---\")\n",
    "print(\"Type: Supervised Regression\")\n",
    "print(\"Goal: Predict the 'range_km' (autonomy) of an EV based on technical specifications.\")\n",
    "print(\"Metrics: RMSE (Root Mean Squared Error), MAE (Mean Absolute Error), R2 Score.\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 4. BASELINE MODEL SELECTION & IMPLEMENTATION\n",
    "# ==========================================\n",
    "print(\"\\n--- 4. Baseline Model ---\")\n",
    "\n",
    "# 4.1 Baseline Naïve (Dummy Regressor - Prédit la moyenne)\n",
    "dummy_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', DummyRegressor(strategy='mean'))\n",
    "])\n",
    "dummy_pipeline.fit(X_train, y_train)\n",
    "y_pred_dummy = dummy_pipeline.predict(X_test)\n",
    "rmse_dummy = np.sqrt(mean_squared_error(y_test, y_pred_dummy))\n",
    "print(f\"Naive Baseline (Dummy Mean) RMSE: {rmse_dummy:.2f} km\")\n",
    "\n",
    "# 4.2 Baseline Forte (Régression Linéaire)\n",
    "lr_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression(n_jobs=-1))\n",
    "])\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "y_pred_lr = lr_pipeline.predict(X_test)\n",
    "\n",
    "# Métriques pour la Régression Linéaire\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"\\nSelected Baseline: Linear Regression Results\")\n",
    "print(f\"MAE:  {mae_lr:.2f} km\")\n",
    "print(f\"RMSE: {rmse_lr:.2f} km\")\n",
    "print(f\"R2:   {r2_lr:.4f}\")\n",
    "\n",
    "# Visualisation de la performance de la baseline\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(y_test, y_pred_lr, alpha=0.6, color='purple')\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2) # Ligne d'identité\n",
    "plt.xlabel('Actual Range (km)')\n",
    "plt.ylabel('Predicted Range (km)')\n",
    "plt.title('Baseline: Actual vs Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca636f9",
   "metadata": {},
   "source": [
    "# LAB5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e82b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\n#######################################################\")\n",
    "print(\"### PART 2 (LAB 5): FEATURE SELECTION & DIMENSIONALITY REDUCTION ###\")\n",
    "print(\"#######################################################\")\n",
    "\n",
    "# ==========================================\n",
    "# 5. FEATURE SELECTION (RFE)\n",
    "# ==========================================\n",
    "print(\"\\n--- 5. Feature Selection using RFE (Recursive Feature Elimination) ---\")\n",
    "\n",
    "# 1. Prepare the data with the preprocessor defined in Part 1\n",
    "# We need to transform the data first to apply RFE\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Get feature names after one-hot encoding\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# 2. Initialize RFE with a base estimator (RandomForest is good for feature importance)\n",
    "# We select the top 10 features\n",
    "rfe_selector = RFE(estimator=RandomForestRegressor(n_jobs=-1, random_state=42), \n",
    "                   n_features_to_select=10, \n",
    "                   step=1)\n",
    "\n",
    "print(\"Fitting RFE (this may take a moment)...\")\n",
    "rfe_selector.fit(X_train_processed, y_train)\n",
    "\n",
    "# 3. Identify Selected Features\n",
    "selected_mask = rfe_selector.support_\n",
    "selected_features = feature_names[selected_mask]\n",
    "\n",
    "print(f\"\\nTop 10 Selected Features by RFE:\")\n",
    "for feature in selected_features:\n",
    "    print(f\" - {feature}\")\n",
    "\n",
    "# 4. Train and Evaluate a model using ONLY selected features\n",
    "# We use LinearRegression again to compare fairly with the Baseline\n",
    "lr_rfe = LinearRegression(n_jobs=-1)\n",
    "lr_rfe.fit(X_train_processed[:, selected_mask], y_train)\n",
    "y_pred_rfe = lr_rfe.predict(X_test_processed[:, selected_mask])\n",
    "\n",
    "rmse_rfe = np.sqrt(mean_squared_error(y_test, y_pred_rfe))\n",
    "r2_rfe = r2_score(y_test, y_pred_rfe)\n",
    "\n",
    "print(f\"\\nModel with RFE Selected Features Results:\")\n",
    "print(f\"RMSE: {rmse_rfe:.2f} km\")\n",
    "print(f\"R2:   {r2_rfe:.4f}\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 6. DIMENSIONALITY REDUCTION (PCA)\n",
    "# ==========================================\n",
    "print(\"\\n--- 6. Dimensionality Reduction using PCA ---\")\n",
    "\n",
    "# 1. Apply PCA\n",
    "# We want to keep 95% of the variance\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "\n",
    "# Fit PCA on the processed training data\n",
    "X_train_pca = pca.fit_transform(X_train_processed)\n",
    "X_test_pca = pca.transform(X_test_processed)\n",
    "\n",
    "n_components = pca.n_components_\n",
    "print(f\"PCA preserved 95% variance with {n_components} components (down from {X_train_processed.shape[1]} features).\")\n",
    "\n",
    "# 2. Visualize Explained Variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA: Explained Variance by Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 3. Train and Evaluate a model using PCA components\n",
    "lr_pca = LinearRegression(n_jobs=-1)\n",
    "lr_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = lr_pca.predict(X_test_pca)\n",
    "\n",
    "rmse_pca = np.sqrt(mean_squared_error(y_test, y_pred_pca))\n",
    "r2_pca = r2_score(y_test, y_pred_pca)\n",
    "\n",
    "print(f\"\\nModel with PCA Results:\")\n",
    "print(f\"RMSE: {rmse_pca:.2f} km\")\n",
    "print(f\"R2:   {r2_pca:.4f}\")\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 7. COMPARISON WITH BASELINE\n",
    "# ==========================================\n",
    "print(\"\\n--- 7. Comparison: Baseline vs Feature Selection vs PCA ---\")\n",
    "\n",
    "# Collect results\n",
    "results_lab5 = pd.DataFrame({\n",
    "    'Model': ['Baseline (All Features)', 'Feature Selection (RFE)', 'Dimensionality Reduction (PCA)'],\n",
    "    'RMSE': [rmse_lr, rmse_rfe, rmse_pca],  # rmse_lr comes from Part 1\n",
    "    'R2': [r2_lr, r2_rfe, r2_pca]          # r2_lr comes from Part 1\n",
    "})\n",
    "\n",
    "print(results_lab5.to_string(index=False))\n",
    "\n",
    "# Visualization Comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Model', y='RMSE', data=results_lab5, palette='magma')\n",
    "plt.title('Model Performance Comparison (RMSE)')\n",
    "plt.ylabel('RMSE (km) - Lower is Better')\n",
    "plt.xticks(rotation=15)\n",
    "for index, row in results_lab5.iterrows():\n",
    "    plt.text(index, row.RMSE + 1, f\"{row.RMSE:.1f}\", color='black', ha=\"center\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
